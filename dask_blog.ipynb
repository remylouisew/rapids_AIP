{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create conda environment - execute in terminal\n",
    "'''\n",
    "conda create -n rapids-0.17 -c rapidsai -c nvidia \\\n",
    "-c conda-forge -c dask -c defaults rapids-blazing=0.17 python=3.7 cudatoolkit=11.0  gcsfs=0.7.1 -y\n",
    "\n",
    "conda activate rapids-0.17\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'rapids_AIP'...\n",
      "remote: Enumerating objects: 155, done.\u001b[K\n",
      "remote: Counting objects: 100% (155/155), done.\u001b[K\n",
      "remote: Compressing objects: 100% (117/117), done.\u001b[K\n",
      "remote: Total 155 (delta 39), reused 131 (delta 27), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (155/155), 107.19 KiB | 2.19 MiB/s, done.\n",
      "Resolving deltas: 100% (39/39), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/remylouisew/rapids_AIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_cudf\n",
    "import cupy as cp\n",
    "import argparse\n",
    "import time\n",
    "import gcsfs\n",
    "import os, json\n",
    "import subprocess\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instatiate LocalCUDACluster to assign dask processes to GPUs\n",
    "\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client, wait\n",
    "\n",
    "cluster = LocalCUDACluster()\n",
    "client = Client(cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions is 10\n"
     ]
    }
   ],
   "source": [
    "# Read dataset into Dask DMatrix from Google Cloud Storage\n",
    "\n",
    "colnames = ['label'] + ['feature-%02d' % i for i in range(1, 29)]\n",
    "train_dir='gs://nvidiadask/higgs1/*.csv' #GCS public bucket\n",
    "df = dask_cudf.read_csv(train_dir, header=None, names=colnames, chunksize=None)\n",
    "\n",
    "print(\"Number of partitions is\", df.npartitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: ------ Files read in --- 1.5120880603790283\n"
     ]
    }
   ],
   "source": [
    "train_dir='gs://nvidiadask/higgs1/*.csv' #GCS public bucket\n",
    "colnames = ['label'] + ['feature-%02d' % i for i in range(1, 29)]\n",
    "start_time = time.time()\n",
    "df = dask_cudf.read_csv(train_dir, header=None, names=colnames, chunksize=None)\n",
    "print(\"[INFO]: ------ Files read in ---\",float(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions is 400\n"
     ]
    }
   ],
   "source": [
    "#REMOVE: TRYING PARQUET\n",
    "\n",
    "colnames = ['label'] + ['feature-%02d' % i for i in range(1, 29)]\n",
    "train_dir='gs://nvidiadask/higgsp.csv' #GCS public bucket\n",
    "df = dask_cudf.read_parquet(train_dir, header=None, names=colnames, chunksize=None)\n",
    "\n",
    "print(\"Number of partitions is\", df.npartitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some basic functions using Dask\n",
    "\n",
    "%%time\n",
    "df[\"key\"] = df.feature02.round()\n",
    "group_means = df.groupby(\"key\").mean().persist()\n",
    "wait(group_means);\n",
    "\n",
    "group_means.head()\n",
    "\n",
    "#group_means.compute() will output the DMatrix as a Pandas DataFrame. A good workflow would be to summarize your data using Dask, then output to pandas for plotting or other pandas functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now restart the kernal so that you can instatiate a new LocalCUDAcluster, \n",
    "# this time one that will spill to host memory when the GPU memory is exceeded\n",
    "\n",
    "import dask_cudf\n",
    "import cupy as cp\n",
    "import argparse\n",
    "import time\n",
    "import gcsfs\n",
    "import dask_cudf\n",
    "import os, json\n",
    "import subprocess\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client, wait\n",
    "from dask.utils import parse_bytes\n",
    "\n",
    "cluster = LocalCUDACluster(CUDA_VISIBLE_DEVICES=\"0\",\n",
    "    rmm_pool_size=parse_bytes(\"15GB\"), # This GPU has 16GB of memory\n",
    "    device_memory_limit=parse_bytes(\"10GB\"),)\n",
    "client = Client(cluster)\n",
    "client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in larger dataset (20GB) from GCS\n",
    "\n",
    "colnames = ['label'] + ['feature-%02d' % i for i in range(1, 29)]\n",
    "train_dir='gs://nvidiadask/higgs2/*.csv' #GCS public bucket\n",
    "df = dask_cudf.read_csv(train_dir, header=None, names=colnames, chunksize=None)\n",
    "\n",
    "print(\"Number of partitions is\", df.npartitions)\n",
    "\n",
    "# Run the dask functions, which will require nearly double the memory available on the GPU\n",
    "%%time\n",
    "df[\"key\"] = df.feature02.round()\n",
    "group_means = df.groupby(\"key\").mean().persist()\n",
    "wait(group_means);\n",
    "\n",
    "group_means.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##spin up notebook with rapids xgboost AND use conda install"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m61",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m61"
  },
  "kernelspec": {
   "display_name": "Python [conda env:rapids-0.17]",
   "language": "python",
   "name": "conda-env-rapids-0.17-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
