{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create conda environment - execute in terminal\n",
    "'''\n",
    "conda create -n rapids-0.17 -c rapidsai -c nvidia \\\n",
    "-c conda-forge -c dask -c defaults rapids-blazing=0.17 python=3.7 cudatoolkit=11.0  gcsfs=0.7.1 -y\n",
    "\n",
    "conda activate rapids-0.17\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'rapids_AIP'...\n",
      "remote: Enumerating objects: 155, done.\u001b[K\n",
      "remote: Counting objects: 100% (155/155), done.\u001b[K\n",
      "remote: Compressing objects: 100% (117/117), done.\u001b[K\n",
      "remote: Total 155 (delta 39), reused 131 (delta 27), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (155/155), 107.19 KiB | 2.19 MiB/s, done.\n",
      "Resolving deltas: 100% (39/39), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/remylouisew/rapids_AIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_cudf\n",
    "import cupy as cp\n",
    "import argparse\n",
    "import time\n",
    "import gcsfs\n",
    "import os, json\n",
    "import subprocess\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instatiate LocalCUDACluster to assign dask processes to GPUs\n",
    "\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client, wait\n",
    "\n",
    "cluster = LocalCUDACluster()\n",
    "client = Client(cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions is 24\n"
     ]
    }
   ],
   "source": [
    "colnames = ['label'] + ['feature-%02d' % i for i in range(1, 29)]\n",
    "train_dir='gs://rwtmp_demo_ml/nvidiadask/folder/*/*.csv' #GCS public bucket\n",
    "df = dask_cudf.read_csv(train_dir, header=None, names=colnames, chunksize=None)\n",
    "\n",
    "print(\"Number of partitions is\", df.npartitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://nvidiadask/folder/**/*.csv'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error at: /opt/conda/envs/rapids-0.17/include/rmm/mr/device/per_device_resource.hpp:134: cudaErrorNoDevice no CUDA-capable device is detected",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a6bdf8fe5209>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcolnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'feature-%02d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m29\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gs://nvidiadask/higgs1/*.csv'\u001b[0m \u001b[0;31m#GCS public bucket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdask_cudf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of partitions is\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids-0.17/lib/python3.7/site-packages/dask_cudf/io/csv.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(path, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"://\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcudf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read_csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"CSV\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_internal_read_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids-0.17/lib/python3.7/site-packages/dask/dataframe/io/csv.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(urlpath, blocksize, lineterminator, compression, sample, enforce, assume_missing, storage_options, include_path_column, **kwargs)\u001b[0m\n\u001b[1;32m    709\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             \u001b[0minclude_path_column\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude_path_column\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m         )\n\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids-0.17/lib/python3.7/site-packages/dask/dataframe/io/csv.py\u001b[0m in \u001b[0;36mread_pandas\u001b[0;34m(reader, urlpath, blocksize, lineterminator, compression, sample, enforce, assume_missing, storage_options, include_path_column, **kwargs)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0;31m# Use sample to infer dtypes and check for presence of include_path_column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0mhead\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minclude_path_column\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minclude_path_column\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhead\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         raise ValueError(\n",
      "\u001b[0;32m/opt/conda/envs/rapids-0.17/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recreate_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids-0.17/lib/python3.7/site-packages/cudf/io/csv.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, lineterminator, quotechar, quoting, doublequote, header, mangle_dupe_cols, usecols, sep, delimiter, delim_whitespace, skipinitialspace, names, dtype, skipfooter, skiprows, dayfirst, compression, thousands, decimal, true_values, false_values, nrows, byte_range, skip_blank_lines, parse_dates, comment, na_values, keep_default_na, na_filter, prefix, index_col, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mna_filter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_filter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     )\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mcudf/_lib/csv.pyx\u001b[0m in \u001b[0;36mcudf._lib.csv.read_csv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error at: /opt/conda/envs/rapids-0.17/include/rmm/mr/device/per_device_resource.hpp:134: cudaErrorNoDevice no CUDA-capable device is detected"
     ]
    }
   ],
   "source": [
    "# Read dataset into Dask DMatrix from Google Cloud Storage\n",
    "col_acq_names = ['LoanID','Channel','SellerName','OrInterestRate','OrUnpaidPrinc','OrLoanTerm',\n",
    "        'OrDate','FirstPayment','OrLTV','OrCLTV','NumBorrow','DTIRat','CreditScore',\n",
    "        'FTHomeBuyer','LoanPurpose','PropertyType','NumUnits','OccStatus','PropertyState',\n",
    "        'Zip','MortInsPerc','ProductType','CoCreditScore','MortInsType','RelMortInd']\n",
    "col_per_names = ['LoanID','MonthRep','Servicer','CurrInterestRate','CAUPB','LoanAge','MonthsToMaturity',\n",
    "          'AdMonthsToMaturity','MaturityDate','MSA','CLDS','ModFlag','ZeroBalCode','ZeroBalDate',\n",
    "          'LastInstallDate','ForeclosureDate','DispositionDate','PPRC','AssetRecCost','MHRC',\n",
    "          'ATFHP','NetSaleProceeds','CreditEnhProceeds','RPMWP','OFP','NIBUPB','PFUPB','RMWPF',\n",
    "          'FPWA','ServicingIndicator']\n",
    "\n",
    "col_acq = ['LoanID','OrDate','OrUnpaidPrinc','Channel','SellerName','PropertyType','NumUnits','PropertyState']\n",
    "col_per = ['LoanID','MonthRep', 'CAUPB','CLDS','ForeclosureDate']\n",
    "\n",
    "parse_dates_acq =['OrDate','FirstPayment']\n",
    "parse_dates_per =['MonthRep','MaturityDate','ZeroBalDate','LastInstallDate','ForeclosureDate','DispositionDate']\n",
    "\n",
    "dtype_acq={ \"LoanID\":\"int\",\"Channel\":\"str\",\"SellerName\":\"str\",\"OrInterestRate\":\"float\",\"OrUnpaidPrinc\":\"float\",\"OrLoanTerm\":\"float\",\"OrDate\":\"str\",\n",
    "   \"FirstPayment\":\"str\",\"OrLTV\":\"float\",\"OrCLTV\":\"float\",  \"NumBorrow\":\"float\", \"DTIRat\":\"float\", \"CreditScore\":\"float\", \"FTHomeBuyer\":\"str\",\n",
    "   \"LoanPurpose\":\"str\", \"PropertyType\":\"str\", \"NumUnits\":\"float\", \"OccStatus\":\"str\",  \"PropertyState\":\"str\",  \"Zip\":\"int\", \"MortInsPerc\":\"float\",\n",
    "   \"ProductType\":\"str\", \"CoCreditScore\":\"float\", \"MortInsType\":\"float\", \"RelMortInd\":\"str\"}\n",
    "\n",
    "dtype_per={\"LoanID\":\"int\",\"MonthRep\":\"str\",\"Servicer\":\"str\", \"CurrInterestRate\":\"float\", \"CAUPB\":\"float\", \"LoanAge\":\"float\",\"MonthsToMaturity\":\"float\",\n",
    "   \"AdMonthsToMaturity\":\"float\", \"MaturityDate\":\"str\", \"MSA\":\"float\", \"CLDS\":\"float\", \"ModFlag\":\"str\", \"ZeroBalCode\":\"float\", \"ZeroBalDate\":\"str\",\n",
    "    \"LastInstallDate\":\"str\",  \"ForeclosureDate\":\"str\", \"DispositionDate\":\"str\", \"PPRC\":\"float\", \"AssetRecCost\":\"float\", \"MHRC\":\"float\", \"ATFHP\":\"float\",\n",
    "    \"NetSaleProceeds\":\"float\", \"CreditEnhProceeds\":\"float\",\"RPMWP\":\"float\",\"OFP\":\"float\",\"NIBUPB\":\"float\", \"PFUPB\":\"float\", \"RMWPF\":\"float\",\n",
    "   \"FPWA\":\"str\", \"ServicingIndicator\":\"str\"\n",
    "}\n",
    "\n",
    "train_dir='gs://nvidiadask/higgs1/*.csv' #GCS public bucket\n",
    "df = dask_cudf.read_csv(train_dir, header=None, names=colnames, chunksize=None)\n",
    "\n",
    "print(\"Number of partitions is\", df.npartitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some basic functions using Dask\n",
    "\n",
    "%%time\n",
    "df[\"key\"] = df.feature02.round()\n",
    "group_means = df.groupby(\"key\").mean().persist()\n",
    "wait(group_means);\n",
    "\n",
    "group_means.head()\n",
    "\n",
    "#group_means.compute() will output the DMatrix as a Pandas DataFrame. A good workflow would be to summarize your data using Dask, then output to pandas for plotting or other pandas functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now restart the kernal so that you can instatiate a new LocalCUDAcluster, \n",
    "# this time one that will spill to host memory when the GPU memory is exceeded\n",
    "\n",
    "import dask_cudf\n",
    "import cupy as cp\n",
    "import argparse\n",
    "import time\n",
    "import gcsfs\n",
    "import dask_cudf\n",
    "import os, json\n",
    "import subprocess\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client, wait\n",
    "from dask.utils import parse_bytes\n",
    "\n",
    "cluster = LocalCUDACluster(CUDA_VISIBLE_DEVICES=\"0\",\n",
    "    rmm_pool_size=parse_bytes(\"15GB\"), # This GPU has 16GB of memory\n",
    "    device_memory_limit=parse_bytes(\"10GB\"),)\n",
    "client = Client(cluster)\n",
    "client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in larger dataset (20GB) from GCS\n",
    "\n",
    "colnames = ['label'] + ['feature-%02d' % i for i in range(1, 29)]\n",
    "train_dir='gs://nvidiadask/higgs2/*.csv' #GCS public bucket\n",
    "df = dask_cudf.read_csv(train_dir, header=None, names=colnames, chunksize=None)\n",
    "\n",
    "print(\"Number of partitions is\", df.npartitions)\n",
    "\n",
    "# Run the dask functions, which will require nearly double the memory available on the GPU\n",
    "%%time\n",
    "df[\"key\"] = df.feature02.round()\n",
    "group_means = df.groupby(\"key\").mean().persist()\n",
    "wait(group_means);\n",
    "\n",
    "group_means.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##spin up notebook with rapids xgboost AND use conda install"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.mnightly-2021-01-20-debian-10-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:mnightly-2021-01-20-debian-10-test"
  },
  "kernelspec": {
   "display_name": "Python [conda env:rapids-0.17]",
   "language": "python",
   "name": "conda-env-rapids-0.17-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
